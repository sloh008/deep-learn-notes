{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "\n",
    "## setup ML app\n",
    "\n",
    "- train/dev (holdout crossvalid)/test\n",
    "\n",
    "    - ratio depends on sample size\n",
    "    \n",
    "    - mismatched dist train/test\n",
    "    \n",
    "        -  rule of thumb dev/ test same dis\n",
    "        \n",
    "    - consider whether a test set is neccessary\n",
    "    \n",
    "        - test set provides unbiased estimate; consider whether that is needed\n",
    "        \n",
    "        - train / dev\n",
    "\n",
    "- hyperparam tuning is an **iterative** process\n",
    "    \n",
    "    - idea &rarr; code &rarr; exp\n",
    "    \n",
    "    - data, input features, comconfigs (gpu/cpu)\n",
    "\n",
    "\n",
    "- bias / variance \n",
    "\n",
    "    - tradeoff not too much concern in DL (???)\n",
    "    \n",
    "    - Diagnosing NN (train vs dev set error; predicated on human performance (**bayes error**))\n",
    "    \n",
    "        - L dev (1%) , H train (11&) &rarr; high variance / overfitting\n",
    "        \n",
    "        - H dev (15%) $\\approx$ H (16%) train &rarr; high bias / underfitting (linear functions)\n",
    "        \n",
    "        - H dev < H train &rarr; high bias, high variance\n",
    "        \n",
    "\n",
    "- Basic ML receipe\n",
    "\n",
    "    - high bias = training set performance\n",
    "    \n",
    "        - larger network\n",
    "        \n",
    "        - train longer\n",
    "        \n",
    "        - optimisation \n",
    "    \n",
    "    - high variance = dev set performance\n",
    "    \n",
    "        - more data\n",
    "        \n",
    "        - regularisation\n",
    "    \n",
    "    \n",
    "## regularisation (high variance problem/overfitting)\n",
    "\n",
    "\n",
    "- **L2 ('weight decay') vs L1**\n",
    "\n",
    "    - λ : hyperparam\n",
    "    \n",
    "    - Smooths decision boundary as a function of λ\n",
    "\n",
    "    - frobenius norm = L2 norm of matrix\n",
    "\n",
    "- How regularisation prevents overfitting\n",
    "\n",
    "    - 'zero out' weights ; simplify nn\n",
    "    \n",
    "    - lambda reduces the range of wl; range z is small; a approaches a linear function\n",
    "    \n",
    "- **dropout**: p(eliminate node) prevents overfitting\n",
    "\n",
    "    - 'inverted dropout'\n",
    "\n",
    "    - intuition: random KO units\n",
    "    \n",
    "        - smaller NN\n",
    "        \n",
    "        - weights spread out, shrink weights a.k.a L2\n",
    "        \n",
    "     - scale keep prob by size of params \n",
    "   \n",
    "    - J undefined\n",
    "    \n",
    "- Other reg tech\n",
    "\n",
    "    - data augmentation\n",
    "    \n",
    "    - early stopping\n",
    "    \n",
    "- Orthogonalisation\n",
    "\n",
    "## Optimisation setup\n",
    "\n",
    "- Normalisation \n",
    "\n",
    "- Vanishing/exploiding gradient with deep networks\n",
    "\n",
    "- Weights initialisation( partial soluton to v/e gradient)\n",
    "\n",
    "    - random: breaks symmetry but 1) slow, especially if random numbers large, 2) high cost at start\n",
    "    \n",
    "    - He/ Xavier initialisation - scale weights; works well for ReLU\n",
    "\n",
    "- gradient checking $dθ_{approx}[i]$\n",
    "\n",
    "    - basically to cross check gradient (derivative) computation\n",
    "\n",
    "    - two-sided difference\n",
    "    \n",
    "    - implementation \n",
    "    \n",
    "        - only debug\n",
    "        \n",
    "        - if fail gradcheck, look at ind comp \n",
    "        \n",
    "        - regularisation term\n",
    "        \n",
    "        - not with dropout\n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
